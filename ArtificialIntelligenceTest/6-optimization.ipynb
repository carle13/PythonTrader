{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimizing the model parameters\n",
        "\n",
        "Now that we have a model and data it's time to train, validate and test our model by optimizing its parameters on our data. Training a model is an iterative process; in each iteration (called an *epoch*). The model makes a guess about the output, calculates the error in its guess (*loss*), collects the derivatives of the error with respect to its parameters (as we saw in the previous module), and **optimizes** these parameters using gradient descent. \n",
        "\n",
        "## Prerequisite code \n",
        "\n",
        "We will load the code from the previous modules on **Datasets & DataLoaders** and **Build Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting hyperparameters \n",
        "\n",
        "Hyperparameters are adjustable parameters that let you control the model optimization process. \n",
        "Different hyperparameter values can impact model training and the level of accuracy.\n",
        "\n",
        "We define the following hyperparameters for training:\n",
        " - **Number of Epochs** - the number times the entire training dataset is pass through the network. \n",
        " - **Batch Size** - the number of data samples seen by the model in each epoch. Iterates are the number of batches needs to compete an epoch.\n",
        " - **Learning Rate** - the size of steps the model match as it searchs for best weights that will produce a higher model accuracy. Smaller values means the model will take a longer time to find the best weights, while larger values may result in the model step over and misses the best weights which yields unpredictable behavior during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add an optimization loop\n",
        "\n",
        "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each \n",
        "iteration of the optimization loop is called an **epoch**. \n",
        "\n",
        "Each epoch consists of two main parts:\n",
        " - **The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.\n",
        " - **The Validation/Test Loop** - iterate over the test dataset to check if model performance is improving.\n",
        "\n",
        "Let's briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to \n",
        "see the `full-impl-label` of the optimization loop.\n",
        "\n",
        "### Add a loss function\n",
        "\n",
        "When presented with some training data, our untrained network is likely not to give the correct \n",
        "answer. **Loss function** measures the degree of dissimilarity of obtained result to the target value, \n",
        "and it is the loss function that we want to minimize during training. To calculate the loss we make a \n",
        "prediction using the inputs of our given data sample and compare it against the true data label value.\n",
        "\n",
        "Common loss functions include:\n",
        "- `nn.MSELoss` (Mean Square Error) used for regression tasks\n",
        "- `nn.NLLLoss` (Negative Log Likelihood) used for classification\n",
        "- `nn.CrossEntropyLoss` combines `nn.LogSoftmax` and `nn.NLLLoss`\n",
        "\n",
        "We pass our model's output logits to `nn.CrossEntropyLoss`, which will normalize the logits and compute the prediction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimization pass\n",
        "\n",
        "Optimization is the process of adjusting model parameters to reduce model error in each training step. **Optimization algorithms** define how this process is performed (in this example we use Stochastic Gradient Descent).\n",
        "All optimization logic is encapsulated in  the ``optimizer`` object. Here, we use the SGD optimizer; additionally, there are many different optimizers\n",
        "available in PyTorch such as `ADAM' and 'RMSProp`, that work better for different kinds of models and data.\n",
        "\n",
        "We initialize the optimizer by registering the model's parameters that need to be trained, and passing in the learning rate hyperparameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inside the training loop, optimization happens in three steps:\n",
        " * Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
        " * Back-propagate the prediction loss with a call to `loss.backwards()`. PyTorch deposits the gradients of the loss w.r.t. each parameter. \n",
        " * Once we have our gradients, we call ``optimizer.step()`` to adjust the parameters by the gradients collected in the backward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full implementation\n",
        "\n",
        "We define `train_loop` that loops over our optimization code, and `test_loop` that \n",
        "evaluates the model's performance against our test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):        \n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        \n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            \n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We initialize the loss function and optimizer, and pass it to `train_loop` and `test_loop`.\n",
        "Feel free to increase the number of epochs to track the model's improving performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.469228  [    0/60000]\n",
            "loss: 1.651474  [ 6400/60000]\n",
            "loss: 1.517545  [12800/60000]\n",
            "loss: 1.787590  [19200/60000]\n",
            "loss: 1.714765  [25600/60000]\n",
            "loss: 1.683866  [32000/60000]\n",
            "loss: 1.667620  [38400/60000]\n",
            "loss: 1.570805  [44800/60000]\n",
            "loss: 1.531351  [51200/60000]\n",
            "loss: 1.840337  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.5%, Avg loss: 0.025662 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.444130  [    0/60000]\n",
            "loss: 1.627747  [ 6400/60000]\n",
            "loss: 1.492218  [12800/60000]\n",
            "loss: 1.771959  [19200/60000]\n",
            "loss: 1.701535  [25600/60000]\n",
            "loss: 1.664293  [32000/60000]\n",
            "loss: 1.653573  [38400/60000]\n",
            "loss: 1.556523  [44800/60000]\n",
            "loss: 1.514603  [51200/60000]\n",
            "loss: 1.826544  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.9%, Avg loss: 0.025392 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.420923  [    0/60000]\n",
            "loss: 1.606188  [ 6400/60000]\n",
            "loss: 1.468808  [12800/60000]\n",
            "loss: 1.757134  [19200/60000]\n",
            "loss: 1.689607  [25600/60000]\n",
            "loss: 1.646439  [32000/60000]\n",
            "loss: 1.640442  [38400/60000]\n",
            "loss: 1.543494  [44800/60000]\n",
            "loss: 1.498791  [51200/60000]\n",
            "loss: 1.813753  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 47.3%, Avg loss: 0.025140 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.398782  [    0/60000]\n",
            "loss: 1.586154  [ 6400/60000]\n",
            "loss: 1.446611  [12800/60000]\n",
            "loss: 1.743985  [19200/60000]\n",
            "loss: 1.678024  [25600/60000]\n",
            "loss: 1.629873  [32000/60000]\n",
            "loss: 1.627672  [38400/60000]\n",
            "loss: 1.531440  [44800/60000]\n",
            "loss: 1.483870  [51200/60000]\n",
            "loss: 1.801931  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 47.5%, Avg loss: 0.024898 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.377010  [    0/60000]\n",
            "loss: 1.567216  [ 6400/60000]\n",
            "loss: 1.425138  [12800/60000]\n",
            "loss: 1.732289  [19200/60000]\n",
            "loss: 1.666919  [25600/60000]\n",
            "loss: 1.614447  [32000/60000]\n",
            "loss: 1.615064  [38400/60000]\n",
            "loss: 1.519355  [44800/60000]\n",
            "loss: 1.469778  [51200/60000]\n",
            "loss: 1.790161  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 47.8%, Avg loss: 0.024662 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.355456  [    0/60000]\n",
            "loss: 1.548515  [ 6400/60000]\n",
            "loss: 1.403926  [12800/60000]\n",
            "loss: 1.721315  [19200/60000]\n",
            "loss: 1.656179  [25600/60000]\n",
            "loss: 1.599552  [32000/60000]\n",
            "loss: 1.602046  [38400/60000]\n",
            "loss: 1.507565  [44800/60000]\n",
            "loss: 1.455868  [51200/60000]\n",
            "loss: 1.778660  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 48.0%, Avg loss: 0.024428 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.333966  [    0/60000]\n",
            "loss: 1.530007  [ 6400/60000]\n",
            "loss: 1.382868  [12800/60000]\n",
            "loss: 1.711411  [19200/60000]\n",
            "loss: 1.645377  [25600/60000]\n",
            "loss: 1.585286  [32000/60000]\n",
            "loss: 1.588727  [38400/60000]\n",
            "loss: 1.495858  [44800/60000]\n",
            "loss: 1.441962  [51200/60000]\n",
            "loss: 1.767398  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 48.2%, Avg loss: 0.024194 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.312376  [    0/60000]\n",
            "loss: 1.511784  [ 6400/60000]\n",
            "loss: 1.361917  [12800/60000]\n",
            "loss: 1.702166  [19200/60000]\n",
            "loss: 1.635024  [25600/60000]\n",
            "loss: 1.571594  [32000/60000]\n",
            "loss: 1.574882  [38400/60000]\n",
            "loss: 1.483945  [44800/60000]\n",
            "loss: 1.427982  [51200/60000]\n",
            "loss: 1.756094  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 48.3%, Avg loss: 0.023961 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.290460  [    0/60000]\n",
            "loss: 1.493338  [ 6400/60000]\n",
            "loss: 1.341241  [12800/60000]\n",
            "loss: 1.693095  [19200/60000]\n",
            "loss: 1.624953  [25600/60000]\n",
            "loss: 1.558401  [32000/60000]\n",
            "loss: 1.560866  [38400/60000]\n",
            "loss: 1.472397  [44800/60000]\n",
            "loss: 1.414071  [51200/60000]\n",
            "loss: 1.745308  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 48.5%, Avg loss: 0.023731 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.269085  [    0/60000]\n",
            "loss: 1.475415  [ 6400/60000]\n",
            "loss: 1.321159  [12800/60000]\n",
            "loss: 1.684100  [19200/60000]\n",
            "loss: 1.615388  [25600/60000]\n",
            "loss: 1.545537  [32000/60000]\n",
            "loss: 1.547053  [38400/60000]\n",
            "loss: 1.460978  [44800/60000]\n",
            "loss: 1.400403  [51200/60000]\n",
            "loss: 1.735510  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 48.6%, Avg loss: 0.023509 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.248694  [    0/60000]\n",
            "loss: 1.457744  [ 6400/60000]\n",
            "loss: 1.301736  [12800/60000]\n",
            "loss: 1.675434  [19200/60000]\n",
            "loss: 1.606176  [25600/60000]\n",
            "loss: 1.533576  [32000/60000]\n",
            "loss: 1.534092  [38400/60000]\n",
            "loss: 1.450795  [44800/60000]\n",
            "loss: 1.387673  [51200/60000]\n",
            "loss: 1.725799  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 48.7%, Avg loss: 0.023299 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.229810  [    0/60000]\n",
            "loss: 1.441109  [ 6400/60000]\n",
            "loss: 1.283596  [12800/60000]\n",
            "loss: 1.667181  [19200/60000]\n",
            "loss: 1.596946  [25600/60000]\n",
            "loss: 1.522490  [32000/60000]\n",
            "loss: 1.521268  [38400/60000]\n",
            "loss: 1.441765  [44800/60000]\n",
            "loss: 1.376091  [51200/60000]\n",
            "loss: 1.716969  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 0.023102 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.212519  [    0/60000]\n",
            "loss: 1.425426  [ 6400/60000]\n",
            "loss: 1.266877  [12800/60000]\n",
            "loss: 1.659043  [19200/60000]\n",
            "loss: 1.587977  [25600/60000]\n",
            "loss: 1.512500  [32000/60000]\n",
            "loss: 1.508952  [38400/60000]\n",
            "loss: 1.434346  [44800/60000]\n",
            "loss: 1.365894  [51200/60000]\n",
            "loss: 1.708498  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 48.9%, Avg loss: 0.022919 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.196970  [    0/60000]\n",
            "loss: 1.410680  [ 6400/60000]\n",
            "loss: 1.251311  [12800/60000]\n",
            "loss: 1.651411  [19200/60000]\n",
            "loss: 1.579983  [25600/60000]\n",
            "loss: 1.503257  [32000/60000]\n",
            "loss: 1.497704  [38400/60000]\n",
            "loss: 1.427650  [44800/60000]\n",
            "loss: 1.356869  [51200/60000]\n",
            "loss: 1.700418  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.0%, Avg loss: 0.022749 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.183048  [    0/60000]\n",
            "loss: 1.397381  [ 6400/60000]\n",
            "loss: 1.237357  [12800/60000]\n",
            "loss: 1.643812  [19200/60000]\n",
            "loss: 1.572682  [25600/60000]\n",
            "loss: 1.494708  [32000/60000]\n",
            "loss: 1.487912  [38400/60000]\n",
            "loss: 1.421890  [44800/60000]\n",
            "loss: 1.348464  [51200/60000]\n",
            "loss: 1.692855  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.1%, Avg loss: 0.022593 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.170601  [    0/60000]\n",
            "loss: 1.385150  [ 6400/60000]\n",
            "loss: 1.224638  [12800/60000]\n",
            "loss: 1.635943  [19200/60000]\n",
            "loss: 1.566270  [25600/60000]\n",
            "loss: 1.486980  [32000/60000]\n",
            "loss: 1.478590  [38400/60000]\n",
            "loss: 1.417557  [44800/60000]\n",
            "loss: 1.340355  [51200/60000]\n",
            "loss: 1.685784  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.2%, Avg loss: 0.022450 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.159194  [    0/60000]\n",
            "loss: 1.374321  [ 6400/60000]\n",
            "loss: 1.213215  [12800/60000]\n",
            "loss: 1.628519  [19200/60000]\n",
            "loss: 1.560254  [25600/60000]\n",
            "loss: 1.480057  [32000/60000]\n",
            "loss: 1.470311  [38400/60000]\n",
            "loss: 1.413482  [44800/60000]\n",
            "loss: 1.332800  [51200/60000]\n",
            "loss: 1.679000  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.3%, Avg loss: 0.022317 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.148793  [    0/60000]\n",
            "loss: 1.364532  [ 6400/60000]\n",
            "loss: 1.202918  [12800/60000]\n",
            "loss: 1.621292  [19200/60000]\n",
            "loss: 1.554490  [25600/60000]\n",
            "loss: 1.473724  [32000/60000]\n",
            "loss: 1.462905  [38400/60000]\n",
            "loss: 1.410055  [44800/60000]\n",
            "loss: 1.326599  [51200/60000]\n",
            "loss: 1.672188  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.5%, Avg loss: 0.022194 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.139428  [    0/60000]\n",
            "loss: 1.355142  [ 6400/60000]\n",
            "loss: 1.193306  [12800/60000]\n",
            "loss: 1.614508  [19200/60000]\n",
            "loss: 1.549139  [25600/60000]\n",
            "loss: 1.468178  [32000/60000]\n",
            "loss: 1.456132  [38400/60000]\n",
            "loss: 1.406862  [44800/60000]\n",
            "loss: 1.321266  [51200/60000]\n",
            "loss: 1.665575  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.6%, Avg loss: 0.022081 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.130531  [    0/60000]\n",
            "loss: 1.346156  [ 6400/60000]\n",
            "loss: 1.184712  [12800/60000]\n",
            "loss: 1.607628  [19200/60000]\n",
            "loss: 1.544088  [25600/60000]\n",
            "loss: 1.462992  [32000/60000]\n",
            "loss: 1.450040  [38400/60000]\n",
            "loss: 1.404178  [44800/60000]\n",
            "loss: 1.316059  [51200/60000]\n",
            "loss: 1.659468  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.6%, Avg loss: 0.021976 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.122267  [    0/60000]\n",
            "loss: 1.337962  [ 6400/60000]\n",
            "loss: 1.176573  [12800/60000]\n",
            "loss: 1.601032  [19200/60000]\n",
            "loss: 1.539385  [25600/60000]\n",
            "loss: 1.458214  [32000/60000]\n",
            "loss: 1.443287  [38400/60000]\n",
            "loss: 1.401886  [44800/60000]\n",
            "loss: 1.311815  [51200/60000]\n",
            "loss: 1.653612  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.5%, Avg loss: 0.021879 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1.114650  [    0/60000]\n",
            "loss: 1.330647  [ 6400/60000]\n",
            "loss: 1.169146  [12800/60000]\n",
            "loss: 1.594767  [19200/60000]\n",
            "loss: 1.535188  [25600/60000]\n",
            "loss: 1.453682  [32000/60000]\n",
            "loss: 1.437060  [38400/60000]\n",
            "loss: 1.399659  [44800/60000]\n",
            "loss: 1.308368  [51200/60000]\n",
            "loss: 1.647707  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.5%, Avg loss: 0.021790 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1.107491  [    0/60000]\n",
            "loss: 1.323767  [ 6400/60000]\n",
            "loss: 1.162527  [12800/60000]\n",
            "loss: 1.588710  [19200/60000]\n",
            "loss: 1.531028  [25600/60000]\n",
            "loss: 1.449410  [32000/60000]\n",
            "loss: 1.431195  [38400/60000]\n",
            "loss: 1.397922  [44800/60000]\n",
            "loss: 1.305093  [51200/60000]\n",
            "loss: 1.642438  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.6%, Avg loss: 0.021707 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 1.100790  [    0/60000]\n",
            "loss: 1.317522  [ 6400/60000]\n",
            "loss: 1.156241  [12800/60000]\n",
            "loss: 1.583018  [19200/60000]\n",
            "loss: 1.527274  [25600/60000]\n",
            "loss: 1.445732  [32000/60000]\n",
            "loss: 1.425458  [38400/60000]\n",
            "loss: 1.396411  [44800/60000]\n",
            "loss: 1.302173  [51200/60000]\n",
            "loss: 1.637390  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.7%, Avg loss: 0.021630 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1.094414  [    0/60000]\n",
            "loss: 1.311917  [ 6400/60000]\n",
            "loss: 1.150630  [12800/60000]\n",
            "loss: 1.577628  [19200/60000]\n",
            "loss: 1.523463  [25600/60000]\n",
            "loss: 1.442154  [32000/60000]\n",
            "loss: 1.419927  [38400/60000]\n",
            "loss: 1.395039  [44800/60000]\n",
            "loss: 1.299684  [51200/60000]\n",
            "loss: 1.632893  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.8%, Avg loss: 0.021559 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 1.088410  [    0/60000]\n",
            "loss: 1.307075  [ 6400/60000]\n",
            "loss: 1.145489  [12800/60000]\n",
            "loss: 1.572401  [19200/60000]\n",
            "loss: 1.520235  [25600/60000]\n",
            "loss: 1.438763  [32000/60000]\n",
            "loss: 1.414945  [38400/60000]\n",
            "loss: 1.393528  [44800/60000]\n",
            "loss: 1.297613  [51200/60000]\n",
            "loss: 1.628369  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.9%, Avg loss: 0.021492 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 1.082726  [    0/60000]\n",
            "loss: 1.302629  [ 6400/60000]\n",
            "loss: 1.140678  [12800/60000]\n",
            "loss: 1.567408  [19200/60000]\n",
            "loss: 1.517165  [25600/60000]\n",
            "loss: 1.435483  [32000/60000]\n",
            "loss: 1.410018  [38400/60000]\n",
            "loss: 1.392451  [44800/60000]\n",
            "loss: 1.295442  [51200/60000]\n",
            "loss: 1.623950  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%, Avg loss: 0.021431 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 1.077018  [    0/60000]\n",
            "loss: 1.298516  [ 6400/60000]\n",
            "loss: 1.136084  [12800/60000]\n",
            "loss: 1.562611  [19200/60000]\n",
            "loss: 1.514237  [25600/60000]\n",
            "loss: 1.432316  [32000/60000]\n",
            "loss: 1.405176  [38400/60000]\n",
            "loss: 1.391396  [44800/60000]\n",
            "loss: 1.293450  [51200/60000]\n",
            "loss: 1.619942  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%, Avg loss: 0.021374 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 1.071483  [    0/60000]\n",
            "loss: 1.294824  [ 6400/60000]\n",
            "loss: 1.131884  [12800/60000]\n",
            "loss: 1.558138  [19200/60000]\n",
            "loss: 1.511255  [25600/60000]\n",
            "loss: 1.429662  [32000/60000]\n",
            "loss: 1.400503  [38400/60000]\n",
            "loss: 1.390279  [44800/60000]\n",
            "loss: 1.291042  [51200/60000]\n",
            "loss: 1.616037  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%, Avg loss: 0.021321 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 1.066221  [    0/60000]\n",
            "loss: 1.291626  [ 6400/60000]\n",
            "loss: 1.128052  [12800/60000]\n",
            "loss: 1.553647  [19200/60000]\n",
            "loss: 1.508542  [25600/60000]\n",
            "loss: 1.426915  [32000/60000]\n",
            "loss: 1.395994  [38400/60000]\n",
            "loss: 1.389654  [44800/60000]\n",
            "loss: 1.289118  [51200/60000]\n",
            "loss: 1.612294  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.1%, Avg loss: 0.021271 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 1.061259  [    0/60000]\n",
            "loss: 1.288697  [ 6400/60000]\n",
            "loss: 1.124291  [12800/60000]\n",
            "loss: 1.549436  [19200/60000]\n",
            "loss: 1.505872  [25600/60000]\n",
            "loss: 1.424368  [32000/60000]\n",
            "loss: 1.391745  [38400/60000]\n",
            "loss: 1.388420  [44800/60000]\n",
            "loss: 1.287297  [51200/60000]\n",
            "loss: 1.608802  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.1%, Avg loss: 0.021224 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 1.056420  [    0/60000]\n",
            "loss: 1.285999  [ 6400/60000]\n",
            "loss: 1.120792  [12800/60000]\n",
            "loss: 1.545298  [19200/60000]\n",
            "loss: 1.503418  [25600/60000]\n",
            "loss: 1.421949  [32000/60000]\n",
            "loss: 1.387535  [38400/60000]\n",
            "loss: 1.386732  [44800/60000]\n",
            "loss: 1.285604  [51200/60000]\n",
            "loss: 1.605239  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.1%, Avg loss: 0.021180 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 1.051845  [    0/60000]\n",
            "loss: 1.283649  [ 6400/60000]\n",
            "loss: 1.117519  [12800/60000]\n",
            "loss: 1.541454  [19200/60000]\n",
            "loss: 1.501107  [25600/60000]\n",
            "loss: 1.419600  [32000/60000]\n",
            "loss: 1.383521  [38400/60000]\n",
            "loss: 1.385059  [44800/60000]\n",
            "loss: 1.283593  [51200/60000]\n",
            "loss: 1.601440  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.1%, Avg loss: 0.021139 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 1.047175  [    0/60000]\n",
            "loss: 1.281364  [ 6400/60000]\n",
            "loss: 1.114412  [12800/60000]\n",
            "loss: 1.537784  [19200/60000]\n",
            "loss: 1.498910  [25600/60000]\n",
            "loss: 1.417513  [32000/60000]\n",
            "loss: 1.379567  [38400/60000]\n",
            "loss: 1.383539  [44800/60000]\n",
            "loss: 1.281937  [51200/60000]\n",
            "loss: 1.598037  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 0.021100 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 1.042633  [    0/60000]\n",
            "loss: 1.279086  [ 6400/60000]\n",
            "loss: 1.111517  [12800/60000]\n",
            "loss: 1.534054  [19200/60000]\n",
            "loss: 1.496717  [25600/60000]\n",
            "loss: 1.415683  [32000/60000]\n",
            "loss: 1.375641  [38400/60000]\n",
            "loss: 1.381959  [44800/60000]\n",
            "loss: 1.279990  [51200/60000]\n",
            "loss: 1.595015  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 0.021063 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 1.038283  [    0/60000]\n",
            "loss: 1.277084  [ 6400/60000]\n",
            "loss: 1.108768  [12800/60000]\n",
            "loss: 1.530848  [19200/60000]\n",
            "loss: 1.494439  [25600/60000]\n",
            "loss: 1.413800  [32000/60000]\n",
            "loss: 1.371778  [38400/60000]\n",
            "loss: 1.380102  [44800/60000]\n",
            "loss: 1.278320  [51200/60000]\n",
            "loss: 1.591932  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 0.021028 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 1.034013  [    0/60000]\n",
            "loss: 1.275140  [ 6400/60000]\n",
            "loss: 1.106162  [12800/60000]\n",
            "loss: 1.527733  [19200/60000]\n",
            "loss: 1.492284  [25600/60000]\n",
            "loss: 1.412122  [32000/60000]\n",
            "loss: 1.368063  [38400/60000]\n",
            "loss: 1.378440  [44800/60000]\n",
            "loss: 1.276760  [51200/60000]\n",
            "loss: 1.588974  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 0.020995 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 1.029699  [    0/60000]\n",
            "loss: 1.273587  [ 6400/60000]\n",
            "loss: 1.103614  [12800/60000]\n",
            "loss: 1.524639  [19200/60000]\n",
            "loss: 1.489976  [25600/60000]\n",
            "loss: 1.411135  [32000/60000]\n",
            "loss: 1.364587  [38400/60000]\n",
            "loss: 1.376851  [44800/60000]\n",
            "loss: 1.274912  [51200/60000]\n",
            "loss: 1.585709  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 0.020964 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 1.025556  [    0/60000]\n",
            "loss: 1.271946  [ 6400/60000]\n",
            "loss: 1.101165  [12800/60000]\n",
            "loss: 1.521825  [19200/60000]\n",
            "loss: 1.487511  [25600/60000]\n",
            "loss: 1.409552  [32000/60000]\n",
            "loss: 1.361282  [38400/60000]\n",
            "loss: 1.375608  [44800/60000]\n",
            "loss: 1.272914  [51200/60000]\n",
            "loss: 1.582919  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 0.020934 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 1.021521  [    0/60000]\n",
            "loss: 1.270321  [ 6400/60000]\n",
            "loss: 1.098778  [12800/60000]\n",
            "loss: 1.519276  [19200/60000]\n",
            "loss: 1.485155  [25600/60000]\n",
            "loss: 1.407945  [32000/60000]\n",
            "loss: 1.357762  [38400/60000]\n",
            "loss: 1.373865  [44800/60000]\n",
            "loss: 1.271142  [51200/60000]\n",
            "loss: 1.580380  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.3%, Avg loss: 0.020905 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 1.017578  [    0/60000]\n",
            "loss: 1.268752  [ 6400/60000]\n",
            "loss: 1.096463  [12800/60000]\n",
            "loss: 1.516796  [19200/60000]\n",
            "loss: 1.482841  [25600/60000]\n",
            "loss: 1.406607  [32000/60000]\n",
            "loss: 1.354697  [38400/60000]\n",
            "loss: 1.372347  [44800/60000]\n",
            "loss: 1.269118  [51200/60000]\n",
            "loss: 1.577963  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.3%, Avg loss: 0.020877 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 1.013643  [    0/60000]\n",
            "loss: 1.267289  [ 6400/60000]\n",
            "loss: 1.094335  [12800/60000]\n",
            "loss: 1.514429  [19200/60000]\n",
            "loss: 1.480541  [25600/60000]\n",
            "loss: 1.405305  [32000/60000]\n",
            "loss: 1.351509  [38400/60000]\n",
            "loss: 1.370632  [44800/60000]\n",
            "loss: 1.267362  [51200/60000]\n",
            "loss: 1.575685  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.3%, Avg loss: 0.020851 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 1.009876  [    0/60000]\n",
            "loss: 1.265858  [ 6400/60000]\n",
            "loss: 1.092310  [12800/60000]\n",
            "loss: 1.512268  [19200/60000]\n",
            "loss: 1.478154  [25600/60000]\n",
            "loss: 1.404059  [32000/60000]\n",
            "loss: 1.348682  [38400/60000]\n",
            "loss: 1.368942  [44800/60000]\n",
            "loss: 1.266019  [51200/60000]\n",
            "loss: 1.573379  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.4%, Avg loss: 0.020825 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 1.006114  [    0/60000]\n",
            "loss: 1.264429  [ 6400/60000]\n",
            "loss: 1.090332  [12800/60000]\n",
            "loss: 1.510063  [19200/60000]\n",
            "loss: 1.475658  [25600/60000]\n",
            "loss: 1.402808  [32000/60000]\n",
            "loss: 1.345886  [38400/60000]\n",
            "loss: 1.367066  [44800/60000]\n",
            "loss: 1.264403  [51200/60000]\n",
            "loss: 1.571229  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.4%, Avg loss: 0.020800 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 1.002517  [    0/60000]\n",
            "loss: 1.263092  [ 6400/60000]\n",
            "loss: 1.088467  [12800/60000]\n",
            "loss: 1.508074  [19200/60000]\n",
            "loss: 1.473225  [25600/60000]\n",
            "loss: 1.401345  [32000/60000]\n",
            "loss: 1.343333  [38400/60000]\n",
            "loss: 1.365039  [44800/60000]\n",
            "loss: 1.262783  [51200/60000]\n",
            "loss: 1.569161  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.4%, Avg loss: 0.020776 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.999054  [    0/60000]\n",
            "loss: 1.261834  [ 6400/60000]\n",
            "loss: 1.086697  [12800/60000]\n",
            "loss: 1.506204  [19200/60000]\n",
            "loss: 1.471119  [25600/60000]\n",
            "loss: 1.400274  [32000/60000]\n",
            "loss: 1.340769  [38400/60000]\n",
            "loss: 1.363432  [44800/60000]\n",
            "loss: 1.260965  [51200/60000]\n",
            "loss: 1.567248  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.5%, Avg loss: 0.020753 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.995534  [    0/60000]\n",
            "loss: 1.260522  [ 6400/60000]\n",
            "loss: 1.084972  [12800/60000]\n",
            "loss: 1.504413  [19200/60000]\n",
            "loss: 1.468855  [25600/60000]\n",
            "loss: 1.399184  [32000/60000]\n",
            "loss: 1.338195  [38400/60000]\n",
            "loss: 1.361832  [44800/60000]\n",
            "loss: 1.259041  [51200/60000]\n",
            "loss: 1.565502  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.5%, Avg loss: 0.020731 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.992110  [    0/60000]\n",
            "loss: 1.259155  [ 6400/60000]\n",
            "loss: 1.083239  [12800/60000]\n",
            "loss: 1.502649  [19200/60000]\n",
            "loss: 1.466559  [25600/60000]\n",
            "loss: 1.398088  [32000/60000]\n",
            "loss: 1.335900  [38400/60000]\n",
            "loss: 1.360137  [44800/60000]\n",
            "loss: 1.257406  [51200/60000]\n",
            "loss: 1.563831  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 0.020709 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.988826  [    0/60000]\n",
            "loss: 1.257825  [ 6400/60000]\n",
            "loss: 1.081626  [12800/60000]\n",
            "loss: 1.500848  [19200/60000]\n",
            "loss: 1.464584  [25600/60000]\n",
            "loss: 1.397193  [32000/60000]\n",
            "loss: 1.333416  [38400/60000]\n",
            "loss: 1.358529  [44800/60000]\n",
            "loss: 1.255759  [51200/60000]\n",
            "loss: 1.562154  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.7%, Avg loss: 0.020688 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.985600  [    0/60000]\n",
            "loss: 1.256374  [ 6400/60000]\n",
            "loss: 1.080085  [12800/60000]\n",
            "loss: 1.499329  [19200/60000]\n",
            "loss: 1.462482  [25600/60000]\n",
            "loss: 1.396371  [32000/60000]\n",
            "loss: 1.331018  [38400/60000]\n",
            "loss: 1.357072  [44800/60000]\n",
            "loss: 1.253860  [51200/60000]\n",
            "loss: 1.560798  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.7%, Avg loss: 0.020667 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.982468  [    0/60000]\n",
            "loss: 1.254971  [ 6400/60000]\n",
            "loss: 1.078590  [12800/60000]\n",
            "loss: 1.497855  [19200/60000]\n",
            "loss: 1.460303  [25600/60000]\n",
            "loss: 1.395919  [32000/60000]\n",
            "loss: 1.328712  [38400/60000]\n",
            "loss: 1.355770  [44800/60000]\n",
            "loss: 1.252426  [51200/60000]\n",
            "loss: 1.559028  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.7%, Avg loss: 0.020648 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.979066  [    0/60000]\n",
            "loss: 1.253650  [ 6400/60000]\n",
            "loss: 1.077123  [12800/60000]\n",
            "loss: 1.496582  [19200/60000]\n",
            "loss: 1.458133  [25600/60000]\n",
            "loss: 1.395097  [32000/60000]\n",
            "loss: 1.326446  [38400/60000]\n",
            "loss: 1.354238  [44800/60000]\n",
            "loss: 1.251080  [51200/60000]\n",
            "loss: 1.557683  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.7%, Avg loss: 0.020629 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.975820  [    0/60000]\n",
            "loss: 1.252367  [ 6400/60000]\n",
            "loss: 1.075723  [12800/60000]\n",
            "loss: 1.495288  [19200/60000]\n",
            "loss: 1.455960  [25600/60000]\n",
            "loss: 1.394469  [32000/60000]\n",
            "loss: 1.324340  [38400/60000]\n",
            "loss: 1.352696  [44800/60000]\n",
            "loss: 1.249785  [51200/60000]\n",
            "loss: 1.556360  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.8%, Avg loss: 0.020610 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.972588  [    0/60000]\n",
            "loss: 1.251081  [ 6400/60000]\n",
            "loss: 1.074422  [12800/60000]\n",
            "loss: 1.493843  [19200/60000]\n",
            "loss: 1.453582  [25600/60000]\n",
            "loss: 1.393651  [32000/60000]\n",
            "loss: 1.322305  [38400/60000]\n",
            "loss: 1.351384  [44800/60000]\n",
            "loss: 1.248514  [51200/60000]\n",
            "loss: 1.555117  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.8%, Avg loss: 0.020592 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.969388  [    0/60000]\n",
            "loss: 1.249766  [ 6400/60000]\n",
            "loss: 1.073178  [12800/60000]\n",
            "loss: 1.492135  [19200/60000]\n",
            "loss: 1.451494  [25600/60000]\n",
            "loss: 1.393026  [32000/60000]\n",
            "loss: 1.320174  [38400/60000]\n",
            "loss: 1.349715  [44800/60000]\n",
            "loss: 1.247103  [51200/60000]\n",
            "loss: 1.553918  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.8%, Avg loss: 0.020575 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.966175  [    0/60000]\n",
            "loss: 1.248520  [ 6400/60000]\n",
            "loss: 1.071860  [12800/60000]\n",
            "loss: 1.490947  [19200/60000]\n",
            "loss: 1.449450  [25600/60000]\n",
            "loss: 1.392312  [32000/60000]\n",
            "loss: 1.318140  [38400/60000]\n",
            "loss: 1.348186  [44800/60000]\n",
            "loss: 1.245636  [51200/60000]\n",
            "loss: 1.552693  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.8%, Avg loss: 0.020557 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.963053  [    0/60000]\n",
            "loss: 1.247239  [ 6400/60000]\n",
            "loss: 1.070635  [12800/60000]\n",
            "loss: 1.489854  [19200/60000]\n",
            "loss: 1.447556  [25600/60000]\n",
            "loss: 1.391711  [32000/60000]\n",
            "loss: 1.316093  [38400/60000]\n",
            "loss: 1.346133  [44800/60000]\n",
            "loss: 1.244117  [51200/60000]\n",
            "loss: 1.551402  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.8%, Avg loss: 0.020540 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.960142  [    0/60000]\n",
            "loss: 1.246020  [ 6400/60000]\n",
            "loss: 1.069488  [12800/60000]\n",
            "loss: 1.488738  [19200/60000]\n",
            "loss: 1.445574  [25600/60000]\n",
            "loss: 1.391161  [32000/60000]\n",
            "loss: 1.314058  [38400/60000]\n",
            "loss: 1.344520  [44800/60000]\n",
            "loss: 1.242425  [51200/60000]\n",
            "loss: 1.550301  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.8%, Avg loss: 0.020524 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.957258  [    0/60000]\n",
            "loss: 1.244792  [ 6400/60000]\n",
            "loss: 1.068385  [12800/60000]\n",
            "loss: 1.487625  [19200/60000]\n",
            "loss: 1.444211  [25600/60000]\n",
            "loss: 1.390507  [32000/60000]\n",
            "loss: 1.312246  [38400/60000]\n",
            "loss: 1.343097  [44800/60000]\n",
            "loss: 1.240962  [51200/60000]\n",
            "loss: 1.549186  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.8%, Avg loss: 0.020508 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.954335  [    0/60000]\n",
            "loss: 1.243525  [ 6400/60000]\n",
            "loss: 1.067331  [12800/60000]\n",
            "loss: 1.486466  [19200/60000]\n",
            "loss: 1.442315  [25600/60000]\n",
            "loss: 1.389928  [32000/60000]\n",
            "loss: 1.310548  [38400/60000]\n",
            "loss: 1.341578  [44800/60000]\n",
            "loss: 1.239875  [51200/60000]\n",
            "loss: 1.548127  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.9%, Avg loss: 0.020492 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.951408  [    0/60000]\n",
            "loss: 1.242558  [ 6400/60000]\n",
            "loss: 1.066245  [12800/60000]\n",
            "loss: 1.485421  [19200/60000]\n",
            "loss: 1.440836  [25600/60000]\n",
            "loss: 1.389096  [32000/60000]\n",
            "loss: 1.308911  [38400/60000]\n",
            "loss: 1.339914  [44800/60000]\n",
            "loss: 1.238495  [51200/60000]\n",
            "loss: 1.547326  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.9%, Avg loss: 0.020477 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.949040  [    0/60000]\n",
            "loss: 1.241233  [ 6400/60000]\n",
            "loss: 1.065214  [12800/60000]\n",
            "loss: 1.484479  [19200/60000]\n",
            "loss: 1.438938  [25600/60000]\n",
            "loss: 1.388347  [32000/60000]\n",
            "loss: 1.307314  [38400/60000]\n",
            "loss: 1.338507  [44800/60000]\n",
            "loss: 1.237166  [51200/60000]\n",
            "loss: 1.546479  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.9%, Avg loss: 0.020462 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.946316  [    0/60000]\n",
            "loss: 1.239960  [ 6400/60000]\n",
            "loss: 1.064217  [12800/60000]\n",
            "loss: 1.483455  [19200/60000]\n",
            "loss: 1.437158  [25600/60000]\n",
            "loss: 1.387530  [32000/60000]\n",
            "loss: 1.305742  [38400/60000]\n",
            "loss: 1.336949  [44800/60000]\n",
            "loss: 1.235762  [51200/60000]\n",
            "loss: 1.545656  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.9%, Avg loss: 0.020447 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.943681  [    0/60000]\n",
            "loss: 1.238696  [ 6400/60000]\n",
            "loss: 1.063260  [12800/60000]\n",
            "loss: 1.482569  [19200/60000]\n",
            "loss: 1.435429  [25600/60000]\n",
            "loss: 1.386716  [32000/60000]\n",
            "loss: 1.304165  [38400/60000]\n",
            "loss: 1.335637  [44800/60000]\n",
            "loss: 1.234563  [51200/60000]\n",
            "loss: 1.544934  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.0%, Avg loss: 0.020432 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.941194  [    0/60000]\n",
            "loss: 1.237452  [ 6400/60000]\n",
            "loss: 1.062237  [12800/60000]\n",
            "loss: 1.481688  [19200/60000]\n",
            "loss: 1.432996  [25600/60000]\n",
            "loss: 1.386177  [32000/60000]\n",
            "loss: 1.302824  [38400/60000]\n",
            "loss: 1.334138  [44800/60000]\n",
            "loss: 1.233323  [51200/60000]\n",
            "loss: 1.544117  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.0%, Avg loss: 0.020418 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.938639  [    0/60000]\n",
            "loss: 1.236308  [ 6400/60000]\n",
            "loss: 1.061285  [12800/60000]\n",
            "loss: 1.480809  [19200/60000]\n",
            "loss: 1.431143  [25600/60000]\n",
            "loss: 1.385389  [32000/60000]\n",
            "loss: 1.301401  [38400/60000]\n",
            "loss: 1.332904  [44800/60000]\n",
            "loss: 1.231999  [51200/60000]\n",
            "loss: 1.543280  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.0%, Avg loss: 0.020404 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.936171  [    0/60000]\n",
            "loss: 1.235005  [ 6400/60000]\n",
            "loss: 1.060278  [12800/60000]\n",
            "loss: 1.479909  [19200/60000]\n",
            "loss: 1.429445  [25600/60000]\n",
            "loss: 1.384662  [32000/60000]\n",
            "loss: 1.299997  [38400/60000]\n",
            "loss: 1.331839  [44800/60000]\n",
            "loss: 1.230502  [51200/60000]\n",
            "loss: 1.542351  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.0%, Avg loss: 0.020390 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.933723  [    0/60000]\n",
            "loss: 1.233677  [ 6400/60000]\n",
            "loss: 1.059295  [12800/60000]\n",
            "loss: 1.479016  [19200/60000]\n",
            "loss: 1.427619  [25600/60000]\n",
            "loss: 1.383841  [32000/60000]\n",
            "loss: 1.298627  [38400/60000]\n",
            "loss: 1.330576  [44800/60000]\n",
            "loss: 1.229172  [51200/60000]\n",
            "loss: 1.541690  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.0%, Avg loss: 0.020377 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.931392  [    0/60000]\n",
            "loss: 1.232254  [ 6400/60000]\n",
            "loss: 1.058424  [12800/60000]\n",
            "loss: 1.478319  [19200/60000]\n",
            "loss: 1.425757  [25600/60000]\n",
            "loss: 1.383153  [32000/60000]\n",
            "loss: 1.297152  [38400/60000]\n",
            "loss: 1.329171  [44800/60000]\n",
            "loss: 1.227945  [51200/60000]\n",
            "loss: 1.540905  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.1%, Avg loss: 0.020363 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.929114  [    0/60000]\n",
            "loss: 1.231017  [ 6400/60000]\n",
            "loss: 1.057616  [12800/60000]\n",
            "loss: 1.477229  [19200/60000]\n",
            "loss: 1.423994  [25600/60000]\n",
            "loss: 1.382494  [32000/60000]\n",
            "loss: 1.295829  [38400/60000]\n",
            "loss: 1.327613  [44800/60000]\n",
            "loss: 1.226936  [51200/60000]\n",
            "loss: 1.540158  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.1%, Avg loss: 0.020350 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.926865  [    0/60000]\n",
            "loss: 1.229741  [ 6400/60000]\n",
            "loss: 1.056659  [12800/60000]\n",
            "loss: 1.476189  [19200/60000]\n",
            "loss: 1.422416  [25600/60000]\n",
            "loss: 1.381866  [32000/60000]\n",
            "loss: 1.294552  [38400/60000]\n",
            "loss: 1.326200  [44800/60000]\n",
            "loss: 1.225934  [51200/60000]\n",
            "loss: 1.539338  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.1%, Avg loss: 0.020337 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.924688  [    0/60000]\n",
            "loss: 1.228511  [ 6400/60000]\n",
            "loss: 1.055791  [12800/60000]\n",
            "loss: 1.475164  [19200/60000]\n",
            "loss: 1.420749  [25600/60000]\n",
            "loss: 1.381249  [32000/60000]\n",
            "loss: 1.293201  [38400/60000]\n",
            "loss: 1.324899  [44800/60000]\n",
            "loss: 1.224989  [51200/60000]\n",
            "loss: 1.538654  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.1%, Avg loss: 0.020325 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.922537  [    0/60000]\n",
            "loss: 1.226490  [ 6400/60000]\n",
            "loss: 1.054960  [12800/60000]\n",
            "loss: 1.474124  [19200/60000]\n",
            "loss: 1.418789  [25600/60000]\n",
            "loss: 1.380623  [32000/60000]\n",
            "loss: 1.291861  [38400/60000]\n",
            "loss: 1.323602  [44800/60000]\n",
            "loss: 1.224016  [51200/60000]\n",
            "loss: 1.537902  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.1%, Avg loss: 0.020312 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.920365  [    0/60000]\n",
            "loss: 1.225344  [ 6400/60000]\n",
            "loss: 1.054118  [12800/60000]\n",
            "loss: 1.473091  [19200/60000]\n",
            "loss: 1.417084  [25600/60000]\n",
            "loss: 1.379967  [32000/60000]\n",
            "loss: 1.290609  [38400/60000]\n",
            "loss: 1.322285  [44800/60000]\n",
            "loss: 1.222925  [51200/60000]\n",
            "loss: 1.537305  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.1%, Avg loss: 0.020300 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.918190  [    0/60000]\n",
            "loss: 1.224050  [ 6400/60000]\n",
            "loss: 1.053294  [12800/60000]\n",
            "loss: 1.471925  [19200/60000]\n",
            "loss: 1.415470  [25600/60000]\n",
            "loss: 1.379305  [32000/60000]\n",
            "loss: 1.289413  [38400/60000]\n",
            "loss: 1.321003  [44800/60000]\n",
            "loss: 1.221834  [51200/60000]\n",
            "loss: 1.536673  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.1%, Avg loss: 0.020287 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.916018  [    0/60000]\n",
            "loss: 1.222712  [ 6400/60000]\n",
            "loss: 1.052512  [12800/60000]\n",
            "loss: 1.470971  [19200/60000]\n",
            "loss: 1.413956  [25600/60000]\n",
            "loss: 1.378589  [32000/60000]\n",
            "loss: 1.288227  [38400/60000]\n",
            "loss: 1.319785  [44800/60000]\n",
            "loss: 1.220825  [51200/60000]\n",
            "loss: 1.536080  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.1%, Avg loss: 0.020276 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.913937  [    0/60000]\n",
            "loss: 1.221519  [ 6400/60000]\n",
            "loss: 1.051780  [12800/60000]\n",
            "loss: 1.470001  [19200/60000]\n",
            "loss: 1.412249  [25600/60000]\n",
            "loss: 1.377964  [32000/60000]\n",
            "loss: 1.286996  [38400/60000]\n",
            "loss: 1.318545  [44800/60000]\n",
            "loss: 1.219909  [51200/60000]\n",
            "loss: 1.535534  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 0.020264 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.911954  [    0/60000]\n",
            "loss: 1.220259  [ 6400/60000]\n",
            "loss: 1.051009  [12800/60000]\n",
            "loss: 1.469104  [19200/60000]\n",
            "loss: 1.410444  [25600/60000]\n",
            "loss: 1.377385  [32000/60000]\n",
            "loss: 1.285919  [38400/60000]\n",
            "loss: 1.317299  [44800/60000]\n",
            "loss: 1.218814  [51200/60000]\n",
            "loss: 1.534844  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 0.020252 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.910020  [    0/60000]\n",
            "loss: 1.219138  [ 6400/60000]\n",
            "loss: 1.050269  [12800/60000]\n",
            "loss: 1.468228  [19200/60000]\n",
            "loss: 1.408732  [25600/60000]\n",
            "loss: 1.376732  [32000/60000]\n",
            "loss: 1.284853  [38400/60000]\n",
            "loss: 1.316120  [44800/60000]\n",
            "loss: 1.217771  [51200/60000]\n",
            "loss: 1.534402  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 0.020241 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.908129  [    0/60000]\n",
            "loss: 1.217939  [ 6400/60000]\n",
            "loss: 1.049549  [12800/60000]\n",
            "loss: 1.467343  [19200/60000]\n",
            "loss: 1.407060  [25600/60000]\n",
            "loss: 1.376064  [32000/60000]\n",
            "loss: 1.283805  [38400/60000]\n",
            "loss: 1.315020  [44800/60000]\n",
            "loss: 1.216811  [51200/60000]\n",
            "loss: 1.533879  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 0.020230 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.906261  [    0/60000]\n",
            "loss: 1.216736  [ 6400/60000]\n",
            "loss: 1.048827  [12800/60000]\n",
            "loss: 1.466428  [19200/60000]\n",
            "loss: 1.405457  [25600/60000]\n",
            "loss: 1.375367  [32000/60000]\n",
            "loss: 1.282724  [38400/60000]\n",
            "loss: 1.313915  [44800/60000]\n",
            "loss: 1.215920  [51200/60000]\n",
            "loss: 1.533408  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 0.020218 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.904505  [    0/60000]\n",
            "loss: 1.215578  [ 6400/60000]\n",
            "loss: 1.048150  [12800/60000]\n",
            "loss: 1.465498  [19200/60000]\n",
            "loss: 1.403862  [25600/60000]\n",
            "loss: 1.374649  [32000/60000]\n",
            "loss: 1.281641  [38400/60000]\n",
            "loss: 1.312748  [44800/60000]\n",
            "loss: 1.215202  [51200/60000]\n",
            "loss: 1.532814  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 0.020207 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.902734  [    0/60000]\n",
            "loss: 1.214377  [ 6400/60000]\n",
            "loss: 1.047382  [12800/60000]\n",
            "loss: 1.464639  [19200/60000]\n",
            "loss: 1.402250  [25600/60000]\n",
            "loss: 1.374001  [32000/60000]\n",
            "loss: 1.280591  [38400/60000]\n",
            "loss: 1.311502  [44800/60000]\n",
            "loss: 1.214270  [51200/60000]\n",
            "loss: 1.532260  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.3%, Avg loss: 0.020196 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.901080  [    0/60000]\n",
            "loss: 1.213315  [ 6400/60000]\n",
            "loss: 1.046677  [12800/60000]\n",
            "loss: 1.463745  [19200/60000]\n",
            "loss: 1.400623  [25600/60000]\n",
            "loss: 1.373248  [32000/60000]\n",
            "loss: 1.279056  [38400/60000]\n",
            "loss: 1.310323  [44800/60000]\n",
            "loss: 1.213228  [51200/60000]\n",
            "loss: 1.531716  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.3%, Avg loss: 0.020186 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.899421  [    0/60000]\n",
            "loss: 1.212217  [ 6400/60000]\n",
            "loss: 1.046022  [12800/60000]\n",
            "loss: 1.462756  [19200/60000]\n",
            "loss: 1.399027  [25600/60000]\n",
            "loss: 1.372518  [32000/60000]\n",
            "loss: 1.278092  [38400/60000]\n",
            "loss: 1.309177  [44800/60000]\n",
            "loss: 1.212220  [51200/60000]\n",
            "loss: 1.531388  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.3%, Avg loss: 0.020175 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.897819  [    0/60000]\n",
            "loss: 1.211235  [ 6400/60000]\n",
            "loss: 1.045378  [12800/60000]\n",
            "loss: 1.461864  [19200/60000]\n",
            "loss: 1.397452  [25600/60000]\n",
            "loss: 1.371649  [32000/60000]\n",
            "loss: 1.277120  [38400/60000]\n",
            "loss: 1.308072  [44800/60000]\n",
            "loss: 1.210970  [51200/60000]\n",
            "loss: 1.530919  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.3%, Avg loss: 0.020165 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.896223  [    0/60000]\n",
            "loss: 1.210164  [ 6400/60000]\n",
            "loss: 1.044670  [12800/60000]\n",
            "loss: 1.461035  [19200/60000]\n",
            "loss: 1.395808  [25600/60000]\n",
            "loss: 1.370885  [32000/60000]\n",
            "loss: 1.276183  [38400/60000]\n",
            "loss: 1.306936  [44800/60000]\n",
            "loss: 1.209866  [51200/60000]\n",
            "loss: 1.530472  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.3%, Avg loss: 0.020154 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.894671  [    0/60000]\n",
            "loss: 1.209204  [ 6400/60000]\n",
            "loss: 1.044071  [12800/60000]\n",
            "loss: 1.460131  [19200/60000]\n",
            "loss: 1.394509  [25600/60000]\n",
            "loss: 1.370117  [32000/60000]\n",
            "loss: 1.275237  [38400/60000]\n",
            "loss: 1.305707  [44800/60000]\n",
            "loss: 1.209096  [51200/60000]\n",
            "loss: 1.530148  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.3%, Avg loss: 0.020144 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.893148  [    0/60000]\n",
            "loss: 1.208192  [ 6400/60000]\n",
            "loss: 1.043479  [12800/60000]\n",
            "loss: 1.459277  [19200/60000]\n",
            "loss: 1.393025  [25600/60000]\n",
            "loss: 1.369323  [32000/60000]\n",
            "loss: 1.274344  [38400/60000]\n",
            "loss: 1.304529  [44800/60000]\n",
            "loss: 1.208191  [51200/60000]\n",
            "loss: 1.529796  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.3%, Avg loss: 0.020134 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.891709  [    0/60000]\n",
            "loss: 1.207146  [ 6400/60000]\n",
            "loss: 1.042870  [12800/60000]\n",
            "loss: 1.458392  [19200/60000]\n",
            "loss: 1.391435  [25600/60000]\n",
            "loss: 1.368593  [32000/60000]\n",
            "loss: 1.273474  [38400/60000]\n",
            "loss: 1.303470  [44800/60000]\n",
            "loss: 1.207359  [51200/60000]\n",
            "loss: 1.529494  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 0.020124 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.890329  [    0/60000]\n",
            "loss: 1.206125  [ 6400/60000]\n",
            "loss: 1.042279  [12800/60000]\n",
            "loss: 1.457483  [19200/60000]\n",
            "loss: 1.389796  [25600/60000]\n",
            "loss: 1.367840  [32000/60000]\n",
            "loss: 1.272591  [38400/60000]\n",
            "loss: 1.302412  [44800/60000]\n",
            "loss: 1.205149  [51200/60000]\n",
            "loss: 1.529117  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 0.020114 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.888951  [    0/60000]\n",
            "loss: 1.205106  [ 6400/60000]\n",
            "loss: 1.041612  [12800/60000]\n",
            "loss: 1.456583  [19200/60000]\n",
            "loss: 1.388336  [25600/60000]\n",
            "loss: 1.367083  [32000/60000]\n",
            "loss: 1.271852  [38400/60000]\n",
            "loss: 1.301400  [44800/60000]\n",
            "loss: 1.204181  [51200/60000]\n",
            "loss: 1.528541  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 0.020104 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.887597  [    0/60000]\n",
            "loss: 1.204090  [ 6400/60000]\n",
            "loss: 1.040911  [12800/60000]\n",
            "loss: 1.455661  [19200/60000]\n",
            "loss: 1.386722  [25600/60000]\n",
            "loss: 1.366370  [32000/60000]\n",
            "loss: 1.271200  [38400/60000]\n",
            "loss: 1.300399  [44800/60000]\n",
            "loss: 1.203335  [51200/60000]\n",
            "loss: 1.527898  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.4%, Avg loss: 0.020095 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.886253  [    0/60000]\n",
            "loss: 1.203019  [ 6400/60000]\n",
            "loss: 1.040274  [12800/60000]\n",
            "loss: 1.454822  [19200/60000]\n",
            "loss: 1.385175  [25600/60000]\n",
            "loss: 1.365488  [32000/60000]\n",
            "loss: 1.270348  [38400/60000]\n",
            "loss: 1.299333  [44800/60000]\n",
            "loss: 1.202463  [51200/60000]\n",
            "loss: 1.527389  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 0.020085 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.885009  [    0/60000]\n",
            "loss: 1.201970  [ 6400/60000]\n",
            "loss: 1.039692  [12800/60000]\n",
            "loss: 1.454014  [19200/60000]\n",
            "loss: 1.383627  [25600/60000]\n",
            "loss: 1.364845  [32000/60000]\n",
            "loss: 1.269697  [38400/60000]\n",
            "loss: 1.298214  [44800/60000]\n",
            "loss: 1.201694  [51200/60000]\n",
            "loss: 1.526837  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 0.020075 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.883784  [    0/60000]\n",
            "loss: 1.201054  [ 6400/60000]\n",
            "loss: 1.039150  [12800/60000]\n",
            "loss: 1.453243  [19200/60000]\n",
            "loss: 1.381783  [25600/60000]\n",
            "loss: 1.364321  [32000/60000]\n",
            "loss: 1.268850  [38400/60000]\n",
            "loss: 1.297175  [44800/60000]\n",
            "loss: 1.200732  [51200/60000]\n",
            "loss: 1.526255  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 0.020066 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.882501  [    0/60000]\n",
            "loss: 1.200033  [ 6400/60000]\n",
            "loss: 1.038630  [12800/60000]\n",
            "loss: 1.452319  [19200/60000]\n",
            "loss: 1.380092  [25600/60000]\n",
            "loss: 1.363906  [32000/60000]\n",
            "loss: 1.268026  [38400/60000]\n",
            "loss: 1.296179  [44800/60000]\n",
            "loss: 1.199797  [51200/60000]\n",
            "loss: 1.525708  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 0.020056 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.881300  [    0/60000]\n",
            "loss: 1.199065  [ 6400/60000]\n",
            "loss: 1.038112  [12800/60000]\n",
            "loss: 1.451486  [19200/60000]\n",
            "loss: 1.378555  [25600/60000]\n",
            "loss: 1.363320  [32000/60000]\n",
            "loss: 1.267403  [38400/60000]\n",
            "loss: 1.295173  [44800/60000]\n",
            "loss: 1.199039  [51200/60000]\n",
            "loss: 1.525273  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 0.020047 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.880164  [    0/60000]\n",
            "loss: 1.198012  [ 6400/60000]\n",
            "loss: 1.037598  [12800/60000]\n",
            "loss: 1.450568  [19200/60000]\n",
            "loss: 1.377087  [25600/60000]\n",
            "loss: 1.362683  [32000/60000]\n",
            "loss: 1.266712  [38400/60000]\n",
            "loss: 1.294121  [44800/60000]\n",
            "loss: 1.198331  [51200/60000]\n",
            "loss: 1.524849  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 0.020038 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.879045  [    0/60000]\n",
            "loss: 1.197076  [ 6400/60000]\n",
            "loss: 1.037060  [12800/60000]\n",
            "loss: 1.449590  [19200/60000]\n",
            "loss: 1.375700  [25600/60000]\n",
            "loss: 1.362065  [32000/60000]\n",
            "loss: 1.265954  [38400/60000]\n",
            "loss: 1.293083  [44800/60000]\n",
            "loss: 1.197405  [51200/60000]\n",
            "loss: 1.524431  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.5%, Avg loss: 0.020029 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 100\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may have noticed that the model is initially not very good (that's OK!). Try running the loop for more `epochs` or adjusting the `learning_rate` to a bigger number. It might also be the case that the model configuration we chose might not be the optimal one for this kind of problem (it isn't). Later courses will delve more into the model shapes that work for vision problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving Models\n",
        "-------------\n",
        "\n",
        "When you are satisfied with the model's performance, you can use `torch.save` to save it. PyTorch models store the learned parameters in an internal state dictionary, called `state_dict`. These can be persisted wit the `torch.save` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"data/model.pth\")\n",
        "\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "c428d4300b96006bd89c7861a64ca1379b2ef13eb8daabe066e18431431e18b4"
    },
    "kernel_info": {
      "name": "conda-env-py38_pytorch-py"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
